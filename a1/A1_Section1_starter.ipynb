{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting and Understanding Word Embedding/Vectors\n",
    "# Using the GloVe Embeddings\n",
    "\n",
    "\n",
    "Word embeddings (also known as word vectors) are a way to encode the meaning of words into a set of numbers.\n",
    "\n",
    "These embeddings are created by training a neural network model using many examples of the use of language.  These examples could be the whole of Wikipedia or a large collection of news articles. \n",
    "\n",
    "To start, we will explore a set of word embeddings that someone else took the time and computational power to create. One of the most commonly-used pre-trained word embeddings are the **GloVe embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings\n",
    "\n",
    "You can read about the GloVe embeddings here: https://nlp.stanford.edu/projects/glove/, and read the original paper describing how they work here: https://nlp.stanford.edu/pubs/glove.pdf.\n",
    "\n",
    "There are several variations of GloVe embeddings. They differ in the text used to train the embedding, and the *size* of the embeddings.\n",
    "\n",
    "Throughout this course we'll use a package called `torchtext`, that is part of PyTorch, that we will be using in most assignments and your project.\n",
    "\n",
    "We'll begin by loading a set of GloVe embeddings. The first time you run the code below, it will cause the download a large file (862MB) containing the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "# The first time you run this will download a ~823MB file\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
    "                              dim=50)    # embedding size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at what the embedding of the word \"apple\" looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it is a torch tensor with dimension `(50,)`. We don't know what the meaning of each number is, but we do know that there are properties of the embeddings that can be observed.  For example, `distances between embeddings` are meaningful.\n",
    "\n",
    "## Measuring Distance\n",
    "\n",
    "Let's consider one specific metric of distance between two embedding vectors called the **Euclidean distance**. The Euclidean distance of two vectors $x = [x_1, x_2, ... x_n]$ and\n",
    "$y = [y_1, y_2, ... y_n]$ is just the 2-norm of their difference $x - y$. We can compute\n",
    "the Euclidean distance between $x$ and $y$: $\\sqrt{\\sum_i (x_i - y_i)^2}$\n",
    "\n",
    "The PyTorch function `torch.norm` computes the 2-norm of a vector for us, so we \n",
    "can compute the Euclidean distance between two vectors like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = glove['cat']\n",
    "y = glove['dog']\n",
    "torch.norm(y - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = glove['apple']\n",
    "b = glove['orange']\n",
    "torch.norm(b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(glove['good'] - glove['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(glove['good'] - glove['water'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(glove['good'] - glove['well'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(glove['good'] - glove['perfect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "An alternative and more commonly-sued measure of distance is the **Cosine Similarity**. The cosine similarity measures the *angle* between two vectors, and has the property that it only considers the *direction* of the vectors, not their the magnitudes. It is computed as follows for two vectors A and B:\n",
    "<img src=\"cosine_sim.png\" width=\"50%\" align = \"middle\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 1., 1.]).unsqueeze(0) # cosine similarity wants at least 2-D inputs\n",
    "y = torch.tensor([2., 2., 2.]).unsqueeze(0)\n",
    "torch.cosine_similarity(x, y) # should be one because x and y point in the same \"direction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity is actually a *similarity* measure rather than a *distance* measure, and gives a result between -1 and 1. Thus, the larger the similarity, (closer to 1) the \"closer in meaning\" the word embeddings are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([-1., -1., -1.]).unsqueeze(0)\n",
    "torch.cosine_similarity(x, z) # should be -1 because x and y point in the opposite \"direction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = glove['cat']\n",
    "y = glove['dog']\n",
    "torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = glove['apple']\n",
    "b = glove['banana']\n",
    "torch.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(glove['good'].unsqueeze(0), \n",
    "                        glove['bad'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(glove['good'].unsqueeze(0), \n",
    "                        glove['well'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(glove['good'].unsqueeze(0), \n",
    "                        glove['perfect'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: torch.cosine_similarity requires two dimensions to work, which is created with the unsqueeze option, illustrated in more detail below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = glove['good']\n",
    "print(x.shape) # [50]\n",
    "y = x.unsqueeze(0) # [1, 50]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "\n",
    "Now that we have notions of distance and similarity in our embedding space, we can talk about words that are \"close\" to each other in the embedding space. For now, let's use Euclidean distances to look at how close various words are to the word \"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = 'cat'\n",
    "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "for w in other:\n",
    "    dist = torch.norm(glove[word] - glove[w]) # euclidean distance\n",
    "    print(w, \"\\t%5.2f\" % float(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'cat'\n",
    "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "for w in other:\n",
    "    dist = torch.cosine_similarity(glove[word].unsqueeze(0),glove[w].unsqueeze(0)) # cosine distance\n",
    "    print(w, \"\\t%5.2f\" % float(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look through the entire **vocabulary** for words that are closest to a point in the embedding space -- for example, we can look for words that are closest to another word such as \"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(vec, n=5):\n",
    "    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    for idx, difference in lst[1:n+1]:                         # take the top n\n",
    "        print(glove.itos[idx], \"\\t%5.2f\" % difference)\n",
    "\n",
    "print_closest_words(glove[\"cat\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['nurse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['elizabeth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['michael'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['health'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['anxiety'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at which words are closest to the midpoints of two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words((glove['happy'] + glove['sad']) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words((glove['lake'] + glove['building']) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words((glove['bravo'] + glove['michael']) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words((glove['one'] + glove['ten']) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "One surprising aspect of word embeddings is that the *directions* in the embedding space can be meaningful. For example, some analogy-like relationships like this tend to hold:\n",
    "\n",
    "$$ king - man + woman \\approx queen $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['king'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top result is a reasonable answer like \"queen\",  and the name of the queen of england.\n",
    "\n",
    "We can flip the analogy around and it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['queen'] - glove['woman'] + glove['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, try a different but related analogies along a gender axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['king'] - glove['prince'] + glove['princess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['uncle'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['grandmother'] - glove['mother'] + glove['father'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['old'] - glove['young'] + glove['father'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also move an embedding towards the direction of \"goodness\" or \"badness\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['good'] - glove['bad'] + glove['programmer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['bad'] - glove['good'] + glove['programmer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias in Word Vectors\n",
    "\n",
    "While it may appear that machine learning models have an implicit air of \"fairness\" about them, because the models\n",
    "make decisions without human intervention. However, models can and do learn whatever bias is present in the training data - in this case the bias is present in the text that the vectors were trained on.\n",
    "\n",
    "Below are some examples that show that the structure of the GloVe vectors encodes the everyday biases present in the texts that they are trained on.\n",
    "\n",
    "We'll start with an example analogy:\n",
    "\n",
    "$$doctor - man + woman \\approx ??$$\n",
    "\n",
    "Using GloVe vectors to find the answer to the above analogy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['doctor'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $$doctor - man + woman \\approx nurse$$ analogy is very concerning.\n",
    "Just to verify, the same result does not appear if we flip the gender terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['doctor'] - glove['woman'] + glove['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see similar types of gender bias with other professions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['programmer'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the first result, none of the other words are even related to\n",
    "programming! In contrast, if we flip the gender terms, we get very\n",
    "different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['programmer'] - glove['woman'] + glove['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results for \"engineer\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['engineer'] - glove['man'] + glove['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_closest_words(glove['engineer'] - glove['woman'] + glove['man'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
