{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca90ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "import torch\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare text using the both the nltk sentence tokenizer (https://www.nltk.org/api/nltk.tokenize.html)\n",
    "# AND the spacy english pipeline (see https://spacy.io/models/en)\n",
    "\n",
    "\n",
    "def prepare_texts(text, min_frequency=3):\n",
    "    \n",
    "    # Get a callable object from spacy that processes the text - lemmatizes and determines part of speech\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Some text cleaning. Do it by sentence, and eliminate punctuation.\n",
    "    lemmas = []\n",
    "    for sent in sent_tokenize(text):  # sent_tokenize separates the sentences \n",
    "        for tok in nlp(sent):         # nlp processes as in Part III\n",
    "            if tok.pos_ not in [\"PUNCT\", \"SPACE\", \"SYM\", \"NUM\", \"X\"] and tok.lemma_ not in \"[]|.,/?'\\\"+-=\":\n",
    "                lemmas.append(tok.lemma_)\n",
    "    \n",
    "    # Count the frequency of each lemmatized word\n",
    "    freqs = Counter()  # word -> occurrence\n",
    "    for w in lemmas:\n",
    "        freqs[w] += 1\n",
    "        \n",
    "    vocab = list(freqs.items())  # List of (word, occurrence)\n",
    "    vocab = sorted(vocab, key=lambda item: item[1], reverse=True)  # Sort by decreasing frequency\n",
    "    \n",
    "    # per Mikolov, don't use the infrequent words, as there isn't much to learn in that case\n",
    "    \n",
    "    frequent_vocab = list(filter(lambda item: item[1]>=min_frequency, vocab))\n",
    "    \n",
    "    # Create the dictionaries to go from word to index or vice-verse\n",
    "    \n",
    "    w2i = {w[0]:i for i,w in enumerate(frequent_vocab)}\n",
    "    i2w = {i:w[0] for i,w in enumerate(frequent_vocab)}\n",
    "    \n",
    "    # Create an Out Of Vocabulary (oov) token as well\n",
    "    w2i[\"<oov>\"] = len(frequent_vocab)\n",
    "    i2w[len(frequent_vocab)] = \"<oov>\"\n",
    "    \n",
    "    # Set all of the words not included in vocabulary nuas oov\n",
    "    filtered_lemmas = []\n",
    "    for lem in lemmas:\n",
    "        if lem not in w2i:\n",
    "            filtered_lemmas.append(\"<oov>\")\n",
    "        else:\n",
    "            filtered_lemmas.append(lem)\n",
    "    \n",
    "    return filtered_lemmas, w2i, i2w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac56a6e",
   "metadata": {},
   "source": [
    "### tokenize_and_preprocess_text creates the training samples for the model. It walks through each word in the corpus, and looks at a window (of size 'window') of words and creates input/output prediction pairs.  We need both positive (in window) samples and negative (out of window) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ba40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preprocess_text(textlist, w2i, window):\n",
    "    \"\"\"\n",
    "    Skip-gram negative sampling: Predict if the target word is in the context.\n",
    "    Uses binary prediction so we need both positive and negative samples\n",
    "    \"\"\"\n",
    "    X, T, Y = [], [], []\n",
    "    \n",
    "    # Tokenize the input\n",
    "    \n",
    "    # TO DO\n",
    "    \n",
    "    # Loop through each token\n",
    "    \n",
    "    # TO DO\n",
    "    \n",
    "    return X, T, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b12f50",
   "metadata": {},
   "source": [
    "## Define Model that will be trained to produce word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b381df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegativeSampling(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        # x: torch.tensor of shape (batch_size), context word\n",
    "        # t: torch.tensor of shape (batch_size), target (\"output\") word.\n",
    "        \n",
    "\n",
    "        # TO DO\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2d1a1",
   "metadata": {},
   "source": [
    "#### The training function - give it the text and it does the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgns(textlist, window, embedding_size):\n",
    "    # Set up a model with Skip-gram with negative sampling (predict context with word)\n",
    "    # textlist: a list of strings\n",
    "    \n",
    "    # Create Training Data \n",
    "    \n",
    "    # Split the training data\n",
    "    \n",
    "    # instantiate the network & set up the optimizer\n",
    "\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962cbc3",
   "metadata": {},
   "source": [
    "### Run Training and retrieve embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d41f77",
   "metadata": {},
   "source": [
    "### Reduce the Dimensionality of Embeddings and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4edc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA #see https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "\n",
    "def visualize_embedding(embedding, most_frequent_from=20, most_frequent_to=80):\n",
    "    print (\"Visualizing the {} to {} most frequent words\".format(most_frequent_from, most_frequent_to))\n",
    "    \n",
    "    # since the embeddings are ordered from most frequent words to least frequent, \n",
    "    # we can easily select a sub range of the most frequent words:\n",
    "    \n",
    "    selected_words = embedding[most_frequent_from:most_frequent_to, :]\n",
    "    \n",
    "    # The function below will reduce a vector to 2 principle components\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    \n",
    "    # Transform the selected embeddings to have 2 dimensions\n",
    "    \n",
    "    embeddings = pca.fit_transform(selected_words)\n",
    "    \n",
    "    # Plot the the reduced embeddings - a point and the word itself\n",
    "    \n",
    "    # TO DO\n",
    "    \n",
    "visualize_embedding(embedding.detach().numpy(), most_frequent_from=20, most_frequent_to=80)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
