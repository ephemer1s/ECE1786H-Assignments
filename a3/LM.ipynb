{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Top-Level Code/Notebook\n",
    "### Training a language model base on Karpathy's minGPT codebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "from pathlib import Path \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.bpe import BPETokenizer \n",
    "from mingpt.utils import set_seed \n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the dataset to train the Language Model (LM)\n",
    "This implementation splits the sentences and so doesn't create training \n",
    "examples that cross sentences.\n",
    "\n",
    "This code is set so that it uses one of two possible datasets, which were also used in Assignment 1: \n",
    "SmallSimpleCorpus.txt or LargerCorpus.txt\n",
    "\n",
    "Arguments:\n",
    "            ds_choice: str. \"small\" or \"large\". (i.e. selects which of the two datasets)\n",
    "            split: str. \"train\" or \"test\".\n",
    "            truncation: int. If -1: no truncation on sentences. Otherwise: truncate to this specific length.\n",
    "\"\"\" \n",
    "\n",
    "class LanguageModelingDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds_choice=\"small\", split=\"train\", truncation=-1):\n",
    "        \n",
    "        base_path = \"./\"\n",
    "        fn = {\"small\": \"SmallSimpleCorpus.txt\", \"large\": \"LargerCorpus.txt\"}\n",
    "        self.ds_choice = ds_choice\n",
    "        self.truncation = truncation  # int. If -1, then\n",
    "        text = Path(base_path, fn[ds_choice]).read_text()\n",
    "        if ds_choice == \"large\":\n",
    "            # Remove the newline char in the middle of sentences\n",
    "            # The \"paragraph splitting\" newlines appear to be \\n\\n -- remove the duplications there\n",
    "            text = text.replace(\"\\n\\n\", \"$$^^$$\").replace(\"\\n\", \" \").replace(\"$$^^$$\", \"\\n\")\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Train / test split\n",
    "        train, val = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "        if split == \"train\":\n",
    "            raw_data = train \n",
    "        else:\n",
    "            raw_data = val \n",
    "\n",
    "        # Tokenize\n",
    "        self.tokenizer = BPETokenizer()\n",
    "        self.data = []  # List of 1-d pytorch tensor\n",
    "        for sent in raw_data:\n",
    "            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n",
    "            if truncation >= 0:\n",
    "                self.data.append(tokenized[:truncation])\n",
    "            else:\n",
    "                self.data.append(tokenized)\n",
    "\n",
    "        # Count some items\n",
    "        self.max_sentence_length = np.max([len(d) for d in self.data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer), \n",
    "        but actually, only a small number of vocab is used, especially for the small text. \n",
    "        \"\"\"\n",
    "        return 50257\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The output should be a tuple x and y, both as pytorch tensors.\n",
    "        Please refer to the `run()` method in the mingpt/trainer.py script for \n",
    "        how the x and y are going to be used.\n",
    "        \"\"\"\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return (x, y)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        \"\"\"\n",
    "        block_size is the size at which lines are truncated to ensure they are equal-length.\n",
    "        \"\"\"\n",
    "        return self.max_sentence_length\n",
    "    \n",
    "# Instantiate the Training Dataset\n",
    "train_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"train\")  # use this for the short corpus\n",
    "#train_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"train\", truncation=512) #use this for long\n",
    "\n",
    "# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)\n",
    "val_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"validation\")\n",
    "#val_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"validation\", truncation=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_collate_fn(batch, device):\n",
    "    x = [item[0] for item in batch]  # List (len B) of varying lengths\n",
    "    y = [item[1] for item in batch]  # List (len B) of the same lengths as x\n",
    "    maxlen = max([len(s) for s in x])\n",
    "\n",
    "    padded_x, padded_y = [], []\n",
    "    for sx, sy in zip(x, y):\n",
    "        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))\n",
    "        padded_y.append(torch.cat([sy, torch.ones(maxlen - len(sy))]))\n",
    "    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  40, 6437,  262, 3290]) tensor([6437,  262, 3290,   13])\n",
      "X:  I rub the dog\n",
      "Y:   rub the dog.\n"
     ]
    }
   ],
   "source": [
    "# Print out an example of the data - this is processed more once it reaches lm_collate_fn (above)\n",
    "x,y = train_dataset[5]\n",
    "print(x, y)\n",
    "print(\"X: \",train_dataset.tokenizer.decode(x))\n",
    "print(\"Y: \",train_dataset.tokenizer.decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.50M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model_config.n_classification_class = 2\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "# Create a Trainer object and set the core hyper-parameters\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 3000  # For small corpus: 3000 iterations is plenty. For large corpus: 100000 iterations is needed\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = 4    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n",
    "trainer = Trainer(train_config, model, train_dataset, val_dataset, collate_fn=lm_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 10.81249\n",
      "iter_dt 19.00ms; iter 100: train loss 5.96995\n",
      "iter_dt 19.00ms; iter 200: train loss 2.49559\n",
      "iter_dt 22.00ms; iter 300: train loss 1.49280\n",
      "iter_dt 19.00ms; iter 400: train loss 0.83902\n",
      "iter_dt 18.00ms; iter 500: train loss 0.78918\n",
      "iter_dt 19.00ms; iter 600: train loss 0.83952\n",
      "iter_dt 19.00ms; iter 700: train loss 0.70429\n",
      "iter_dt 21.00ms; iter 800: train loss 0.64494\n",
      "iter_dt 19.01ms; iter 900: train loss 0.59071\n",
      "iter_dt 18.00ms; iter 1000: train loss 0.56029\n",
      "iter_dt 19.00ms; iter 1100: train loss 0.76987\n",
      "iter_dt 19.00ms; iter 1200: train loss 0.58646\n",
      "iter_dt 19.02ms; iter 1300: train loss 0.61791\n",
      "iter_dt 19.00ms; iter 1400: train loss 0.66156\n",
      "iter_dt 19.00ms; iter 1500: train loss 0.68874\n",
      "iter_dt 19.01ms; iter 1600: train loss 0.69681\n",
      "iter_dt 19.00ms; iter 1700: train loss 0.62078\n",
      "iter_dt 19.09ms; iter 1800: train loss 0.58298\n",
      "iter_dt 18.34ms; iter 1900: train loss 0.59302\n",
      "iter_dt 19.02ms; iter 2000: train loss 0.59085\n",
      "iter_dt 19.01ms; iter 2100: train loss 0.60756\n",
      "iter_dt 23.01ms; iter 2200: train loss 0.64772\n",
      "iter_dt 19.88ms; iter 2300: train loss 0.57708\n",
      "iter_dt 19.01ms; iter 2400: train loss 0.62975\n",
      "iter_dt 19.06ms; iter 2500: train loss 0.65382\n",
      "iter_dt 18.01ms; iter 2600: train loss 0.64882\n",
      "iter_dt 18.91ms; iter 2700: train loss 0.76117\n",
      "iter_dt 19.24ms; iter 2800: train loss 0.63148\n",
      "iter_dt 19.02ms; iter 2900: train loss 0.68538\n"
     ]
    }
   ],
   "source": [
    "# This function is called at the end of every batch in training\n",
    "# and is used to report the amount of time per 100 batches, and the loss at that point\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "# Train!\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(trainer.device)\n",
    "# store the saved model in a file, so can re-use later\n",
    "modelsavename= \"model_filename.pt\"  # change the name here to save in a specific file (and restore below)\n",
    "with open(modelsavename, \"wb\") as f:\n",
    "    torch.save(trainer.model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained language model to predict a sequence of words following a few words\n",
    "encoded_prompt = train_dataset.tokenizer(\"He and I\").to(trainer.device)\n",
    "generated_sequence, idx, probs = trainer.model.modified_generate(encoded_prompt, trainer.device, temperature=0.8, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He and I hold the dog.. cat. cat and dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hold  0.491</td>\n",
       "      <td>the  0.525</td>\n",
       "      <td>dog  0.564</td>\n",
       "      <td>.  0.996</td>\n",
       "      <td>.  0.953</td>\n",
       "      <td>cat  0.772</td>\n",
       "      <td>.  0.841</td>\n",
       "      <td>cat  0.720</td>\n",
       "      <td>and  0.647</td>\n",
       "      <td>dog  0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rub  0.262</td>\n",
       "      <td>a  0.472</td>\n",
       "      <td>cat  0.435</td>\n",
       "      <td>.  0.003</td>\n",
       "      <td>.  0.013</td>\n",
       "      <td>dog  0.224</td>\n",
       "      <td>and  0.156</td>\n",
       "      <td>dog  0.269</td>\n",
       "      <td>.  0.350</td>\n",
       "      <td>cat  0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can  0.246</td>\n",
       "      <td>and  0.002</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>cat  0.013</td>\n",
       "      <td>a  0.001</td>\n",
       "      <td>a  0.002</td>\n",
       "      <td>and  0.005</td>\n",
       "      <td>can  0.001</td>\n",
       "      <td>rub  0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holds  0.001</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>dog  0.012</td>\n",
       "      <td>the  0.001</td>\n",
       "      <td>the  0.001</td>\n",
       "      <td>a  0.003</td>\n",
       "      <td>rub  0.001</td>\n",
       "      <td>can  0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and  0.000</td>\n",
       "      <td>hold  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>a  0.004</td>\n",
       "      <td>and  0.001</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>the  0.002</td>\n",
       "      <td>holds  0.001</td>\n",
       "      <td>holds  0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>dog  0.000</td>\n",
       "      <td>the  0.003</td>\n",
       "      <td>hold  0.000</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>.  0.001</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>hold  0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1            2            3            4  \\\n",
       "0    hold  0.491     the  0.525   dog  0.564     .  0.996     .  0.953   \n",
       "1     rub  0.262       a  0.472   cat  0.435     .  0.003     .  0.013   \n",
       "2     can  0.246     and  0.002     a  0.000   and  0.000   cat  0.013   \n",
       "3   holds  0.001     cat  0.000     .  0.000   cat  0.000   dog  0.012   \n",
       "4     and  0.000    hold  0.000   and  0.000   rub  0.000     a  0.004   \n",
       "5     cat  0.000   holds  0.000   the  0.000   dog  0.000   the  0.003   \n",
       "\n",
       "              5            6            7              8              9  \n",
       "0    cat  0.772     .  0.841   cat  0.720     and  0.647     dog  0.976  \n",
       "1    dog  0.224   and  0.156   dog  0.269       .  0.350     cat  0.013  \n",
       "2      a  0.001     a  0.002   and  0.005     can  0.001     rub  0.008  \n",
       "3    the  0.001   the  0.001     a  0.003     rub  0.001     can  0.001  \n",
       "4    and  0.001     .  0.000   the  0.002   holds  0.001   holds  0.001  \n",
       "5   hold  0.000   cat  0.000     .  0.001       a  0.000    hold  0.000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = []\n",
    "for i, x in enumerate(idx):\n",
    "    col = []\n",
    "    for j, y in enumerate(x):\n",
    "        col.append(train_dataset.tokenizer.decode(idx[i][j].reshape(1)) + f\"  {probs[i][j]:.3f}\")\n",
    "    word.append(col)\n",
    "    \n",
    "import pandas as pd\n",
    "result = pd.DataFrame(data=word)\n",
    "result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tokenizer.decode(generated_sequence[0][0].reshape(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
    "generated_sequence, idx, probs = trainer.model.modified_generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She rubs a cat and dog. cat. cat. cat'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a  0.442</td>\n",
       "      <td>cat  0.535</td>\n",
       "      <td>and  0.665</td>\n",
       "      <td>dog  0.999</td>\n",
       "      <td>.  0.999</td>\n",
       "      <td>cat  0.683</td>\n",
       "      <td>.  0.551</td>\n",
       "      <td>cat  0.684</td>\n",
       "      <td>.  0.992</td>\n",
       "      <td>cat  0.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the  0.370</td>\n",
       "      <td>dog  0.465</td>\n",
       "      <td>.  0.335</td>\n",
       "      <td>cat  0.001</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>dog  0.317</td>\n",
       "      <td>and  0.435</td>\n",
       "      <td>dog  0.315</td>\n",
       "      <td>.  0.008</td>\n",
       "      <td>dog  0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and  0.188</td>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>can  0.006</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>rub  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>holds  0.005</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>can  0.000</td>\n",
       "      <td>can  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>rub  0.003</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>holds  0.000</td>\n",
       "      <td>a  0.000</td>\n",
       "      <td>the  0.000</td>\n",
       "      <td>s  0.000</td>\n",
       "      <td>I  0.000</td>\n",
       "      <td>.  0.000</td>\n",
       "      <td>and  0.000</td>\n",
       "      <td>s  0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1              2              3            4  \\\n",
       "0       a  0.442     cat  0.535     and  0.665     dog  0.999     .  0.999   \n",
       "1     the  0.370     dog  0.465       .  0.335     cat  0.001     .  0.000   \n",
       "2     and  0.188   holds  0.000       a  0.000   holds  0.000   and  0.000   \n",
       "3       .  0.000       .  0.000     the  0.000     rub  0.000   rub  0.000   \n",
       "4     cat  0.000       .  0.000     can  0.000     can  0.000     a  0.000   \n",
       "5   holds  0.000     and  0.000   holds  0.000       a  0.000   the  0.000   \n",
       "\n",
       "             5              6            7            8            9  \n",
       "0   cat  0.683       .  0.551   cat  0.684     .  0.992   cat  0.607  \n",
       "1   dog  0.317     and  0.435   dog  0.315     .  0.008   dog  0.393  \n",
       "2     a  0.000     can  0.006   and  0.000   cat  0.000     a  0.000  \n",
       "3   and  0.000   holds  0.005     a  0.000     a  0.000   the  0.000  \n",
       "4   the  0.000     rub  0.003   the  0.000   the  0.000   and  0.000  \n",
       "5     s  0.000       I  0.000     .  0.000   and  0.000     s  0.000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = []\n",
    "for i, x in enumerate(idx):\n",
    "    col = []\n",
    "    for j, y in enumerate(x):\n",
    "        col.append(train_dataset.tokenizer.decode(idx[i][j].reshape(1)) + f\"  {probs[i][j]:.3f}\")\n",
    "    word.append(col)\n",
    "    \n",
    "import pandas as pd\n",
    "result = pd.DataFrame(data=word)\n",
    "result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 0.4790, 0.5130, 0.7420, 1.0000, 1.0000, 0.8940,\n",
       "         0.5980, 1.0000, 0.9990, 0.9730]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(probs, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below shows how to reload the model from the saved file; is useful things that take long to train\n",
    "model.load_state_dict(torch.load(modelsavename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing how the reloaded model still works\n",
    "encoded_prompt = train_dataset.tokenizer(\"I hold\").to(trainer.device)\n",
    "generated_sequence, probs = trainer.model.modified_generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hold a cat and dog.. cat. cat.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tokenizer.decode(generated_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.3860, 0.5210, 0.7890, 0.9990, 1.0000, 0.8280, 0.6710,\n",
       "         0.9970, 0.6310, 0.9970]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(probs, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c245645368b405f9e41f3dedb59d0df7c5d5feced548513488e8eb3fe8134cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
